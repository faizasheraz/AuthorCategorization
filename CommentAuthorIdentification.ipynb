{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn0MlMfhV4mbZgEDJ77ElX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faizasheraz/AuthorCategorization/blob/main/CommentAuthorIdentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification of authors of reddit comments**\n"
      ],
      "metadata": {
        "id": "6tdNgKZzvL1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sctj4c2nLKG",
        "outputId": "897a6be4-f389-4269-8cbf-93b04b432226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8I1cYJwEI0T",
        "outputId": "52d4c602-41b4-421b-f7d2-932a125d2fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0         author        subreddit  \\\n",
            "0      278137  ThisIs_MyName  ProgrammerHumor   \n",
            "1      277405  ThisIs_MyName        Economics   \n",
            "2      277404  ThisIs_MyName      programming   \n",
            "3      277397  ThisIs_MyName   instant_regret   \n",
            "4      277396  ThisIs_MyName        Economics   \n",
            "5      277395  ThisIs_MyName  ProgrammerHumor   \n",
            "6      277394  ThisIs_MyName      programming   \n",
            "7      277412  ThisIs_MyName      programming   \n",
            "8      277410  ThisIs_MyName      programming   \n",
            "9      277393  ThisIs_MyName            anime   \n",
            "\n",
            "                                                body  \n",
            "0  Works sometimes... Try `System.out.printf(\"u n...  \n",
            "1                               the `/s` is implicit  \n",
            "2                     Are you taking about Postgres?  \n",
            "3                                                Aww  \n",
            "4  Not sure what you're asking. US-based companie...  \n",
            "5                        How can structs have holes?  \n",
            "6  Interesting, what a weird implementation of fo...  \n",
            "7  It changes the hash though which prevents ever...  \n",
            "8  Looks like the artists define derivatives of s...  \n",
            "9  Just make a throwaway. I'm known as [Elmo Bagg...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Data Cleaning\n",
        "#----------------\n",
        "\n",
        "import re\n",
        "import statistics\n",
        "import sys\n",
        "\n",
        "file_name = \"/content/drive/My Drive/workspace/ColabNotebooks/top_author_comments.csv\"\n",
        "\n",
        "author_subreddits_comments_df = pd.read_csv(file_name)\n",
        "print(author_subreddits_comments_df.head(10))\n",
        "\n",
        "author_list = set(author_subreddits_comments_df[\"author\"])\n",
        "\n",
        "#REMOVING \n",
        "#URLs (with slashes, .coms, www.)\n",
        "# text like <f0><U+361 .. > # example : <f0><U+009>\n",
        "#non alphanumeric characters \n",
        "\n",
        "author_subreddits_comments_df[\"body\"] = author_subreddits_comments_df[\"body\"].str.replace(r\"/r/\\.*\\S+\", \" \")\n",
        "author_subreddits_comments_df[\"body\"] = author_subreddits_comments_df[\"body\"].str.replace(r\"https?:/\\/\\.*\\S+\", \" \")\n",
        "author_subreddits_comments_df[\"body\"] = author_subreddits_comments_df[\"body\"].str.replace(r\"(<f0>)?<U\\+\\S+\", \" \")\n",
        "author_subreddits_comments_df[\"body\"] = author_subreddits_comments_df[\"body\"].str.replace(r\"[^a-zA-Z\\d\\s']\", \" \")\n",
        "\n",
        "#removing rows with duplicate comments\n",
        "author_subreddits_comments_df.drop_duplicates(subset=\"body\", keep=\"first\", inplace=True)\n",
        "author_subreddits_comments_df.sort_values(\"author\", inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GMCk08j7Rro8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f2a125-2527-479c-fb1b-f395572cbb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum_comments 2885\n"
          ]
        }
      ],
      "source": [
        "#Selecting the same number of comments for the model\n",
        "# Balance of comments of different authors\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "min_comments = len(author_subreddits_comments_df[author_subreddits_comments_df[\"author\"] == list(author_list)[0]]) \n",
        "author_dfs = []\n",
        "for author in author_list:\n",
        "  df = author_subreddits_comments_df[author_subreddits_comments_df[\"author\"] == author]\n",
        "  author_dfs.append(df)\n",
        "  if(len(df) < min_comments):\n",
        "    min_comments = len(df)\n",
        "print(\"minimum_comments\" , min_comments)\n",
        "\n",
        "# For now taking only equal amount of comments for each author which is the minimum comments by an author\n",
        "author_dfs = [df.head(min_comments) for df in author_dfs]\n",
        "author_subreddits_comments_df = pd.concat(author_dfs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXaJXrNtTfFo",
        "outputId": "804f0c11-37c9-4885-e4a7-db57e92d8c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       meatduck12\n",
            "1         awhaling\n",
            "2         awhaling\n",
            "3    ThisIs_MyName\n",
            "4    ThisIs_MyName\n",
            "5    ThisIs_MyName\n",
            "6         awhaling\n",
            "7       meatduck12\n",
            "8    ThisIs_MyName\n",
            "9       meatduck12\n",
            "Name: author, dtype: object\n",
            "{'meatduck12': 0, 'ThisIs_MyName': 1, 'awhaling': 2}\n",
            "[0, 2, 2, 1, 1, 1, 2, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "#  Shuffling dataset and converting authors(labels) to integar form for the \n",
        "#  neural network\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "#shuffling entries of dataframe so that comments of one author are not together as a group\n",
        "author_subreddits_comments_df = author_subreddits_comments_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#getting authors corresponding to comments of authors\n",
        "comment_authors_list = author_subreddits_comments_df[\"author\"]\n",
        "print(comment_authors_list[0:10])\n",
        "\n",
        "# map authors of comments(our labels) to integars\n",
        "dict_all_authors = dict(zip(author_list, list(range(0,len(author_list)))))\n",
        "print(dict_all_authors)\n",
        "\n",
        "int_comment_authors_list = [dict_all_authors[author] for author in comment_authors_list]\n",
        "print(int_comment_authors_list[0:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCJDMZHRYWUg",
        "outputId": "4a682d53-86ae-4d87-e4ca-a945a29b8dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['incentivized', 'TH', 'complications', 'tossing', 'initial', 'soldiers', 'schools', 'showing', '24', 'Racoon']\n",
            "[['If', 'the', 'federal', 'government', 'can', 'regulate', 'civil', 'rights', 'they', 'should', 'be', 'able', 'to', 'regulate', 'this', 'shit', 'too', 'This', 'is', 'getting', 'absurd'], [\"That's\", 'not', 'remotely', 'funny'], ['The', 'only', 'time', 'it', 'made', 'sense', 'was', 'when', 'the', 'uprising', 'event', 'game', 'out', 'and', 'I', 'guess', 'if', 'you', 'wanted', 'to', 'see', 'it', 'up', 'close', 'But', 'yeah', 'how', 'stupid', 'is', 'that'], ['I', 'call', 'BS'], [\"You're\", 'talking', 'about', 'pinch', 'collars', 'that', 'stop', 'dogs', 'from', 'pulling', 'the', 'leash', 'and', 'strangling', 'themselves', 'The', 'guy', \"you're\", 'replying', 'to', 'is', 'talking', 'about', 'spiked', 'collars', 'that', 'protect', 'the', 'animal', 'from', 'predators'], ['Yes', 'use', 'the', 'rotate', 'buttons', 'on', 'the', 'top', 'left', \"I'm\", 'pretty', 'sure', 'there', 'is', 'a', 'hotkey', 'too', 'Of', 'course', 'you', 'need', 'RES', \"Don't\", 'browse', 'without', 'it'], ['So', 'is', 'this', 'gif', \"It's\", 'hilariously', 'accurate'], ['In', 'the', 'current', 'day', 'and', 'age', 'any', 'college', 'will', 'be', 'LGBT', 'friendly', 'Another', 'word', 'of', 'advice', 'watch', 'out', 'when', 'colleges', 'tell', 'you', 'they', 'will', 'meet', '100', 'of', 'needed', 'financial', 'aid', 'This', 'means', 'you', 'will', 'still', 'have', 'to', 'contribute', 'whatever', 'FAFSA', 'says', 'you', 'have', 'to', 'and', 'the', 'aid', 'they', 'give', 'you', 'could', 'come', 'in', 'the', 'form', 'of', 'loans', 'which', 'you', 'can', 'get', 'anywhere'], ['Chu2', 'Chuunibyou', 'demo', 'Koi', 'ga', 'Shitai', 'Love', 'Chunibyo', 'amp', 'amp', 'amp', 'amp', 'Other', 'Delusions'], ['Expecting', 'Harden', 'to', 'spend', 'a', 'good', 'amount', 'of', 'time', 'on', 'ball', 'because', 'of', 'familiarity', 'Paul', 'can', 'be', 'the', 'go', 'score', 'for', 'me', 'guy', 'like', 'Wade', 'to', 'LeBron', 'and', 'also', 'handle', 'the', 'ball', 'when', 'Harden', 'is', 'tired', 'On', 'defense', 'Harden', 'can', 'defend', 'the', 'SG']]\n",
            "[[14088, 10241, 14569, 11818, 5539, 15284, 7977, 13186, 12931, 15891, 15081, 13329, 14820, 15284, 1725, 7434, 10383, 2708, 3138, 661, 1003], [5602, 1288, 3208, 901], [4990, 2293, 7828, 13568, 6358, 9594, 11303, 1291, 10241, 3301, 2244, 9455, 14744, 12558, 4598, 15179, 1602, 9541, 16015, 14820, 12823, 13568, 3565, 4926, 8387, 6998, 10438, 3621, 3138, 15347], [4598, 919, 12415], [6163, 799, 13716, 16236, 16229, 15347, 15080, 15229, 583, 12443, 10241, 14520, 12558, 12984, 3190, 4990, 12888, 2644, 10835, 14820, 3138, 799, 13716, 5040, 16229, 15347, 15274, 10241, 5596, 583, 946], [7661, 1153, 10241, 12951, 5747, 9495, 10241, 15988, 2721, 4088, 10299, 6646, 4476, 3138, 9780, 2264, 10383, 7870, 4607, 9541, 10494, 9454, 14788, 13262, 11070, 13568], [15507, 3138, 1725, 14202, 7198, 12820, 9268], [6770, 10241, 5624, 1530, 12558, 7750, 2222, 8051, 8447, 15081, 13921, 4749, 12032, 1546, 540, 16203, 2194, 14744, 1291, 1201, 1647, 9541, 12931, 8447, 14087, 4457, 540, 10169, 2250, 3897, 2708, 10312, 9541, 8447, 11174, 475, 14820, 1990, 6344, 9930, 7418, 9541, 475, 14820, 12558, 10241, 3897, 12931, 6740, 9541, 13081, 3475, 15262, 10241, 5584, 540, 8422, 1330, 9541, 5539, 6021, 5296], [15935, 2392, 13112, 14197, 12594, 15089, 8297, 12213, 6122, 6122, 6122, 6122, 1529, 9498], [12699, 11670, 14820, 14142, 9780, 11429, 6640, 540, 7828, 9495, 10342, 8381, 540, 14604, 6077, 5539, 15081, 10241, 14572, 11873, 15415, 1374, 12888, 14329, 15686, 14820, 3993, 12558, 11776, 16190, 10241, 10342, 1291, 11670, 3138, 13125, 14905, 8246, 11670, 5539, 15272, 10241, 12518]]\n"
          ]
        }
      ],
      "source": [
        "#  Converting input data (comments) to integars for the neural network\n",
        "#\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "import statistics\n",
        "import sys \n",
        "\n",
        "#tokenize comments text into words\n",
        "comments_list = author_subreddits_comments_df[\"body\"]\n",
        "tokenized_comments_list = [comment.split() for comment in comments_list] \n",
        "\n",
        "#print(comments_list[0:3])\n",
        "#print(tokenized_comments_list[0:3])\n",
        "\n",
        "# Get words vocabury from comments and map to integars \n",
        "all_words = []\n",
        "for comment in tokenized_comments_list:\n",
        "  all_words += comment\n",
        "all_words = set(all_words)\n",
        "\n",
        "dict_all_words = dict(zip(all_words, list(range(1, len(all_words)+1))))\n",
        "print(list(dict_all_words)[:10])\n",
        "\n",
        "#get integars list of tokenized comments\n",
        "int_tokenized_comments_list = [[dict_all_words[word] for word in comment] for comment in tokenized_comments_list]\n",
        "print(tokenized_comments_list[0:10])\n",
        "print(int_tokenized_comments_list[0:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpwoZsN5a-QD",
        "outputId": "ec67bf9b-b0e9-4223-d741-10f579c576fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:  23.344425187752744\n",
            "median:  14\n",
            "max 1055\n",
            "223\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Getting an idea of average comment lengths to get rid of comments that are too\n",
        "# long or too short as compared to mean and median\n",
        "#\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "comments_words_len = [len(tokenized_comment) for tokenized_comment in tokenized_comments_list]\n",
        "mean = statistics.mean(comments_words_len)\n",
        "median = statistics.median(comments_words_len)\n",
        "print(\"mean: \", mean)\n",
        "print(\"median: \", median)\n",
        "print(\"max\", max(comments_words_len))\n",
        "\n",
        "count = 0\n",
        "for length in comments_words_len:\n",
        "  if length > 100:\n",
        "    count = (count+1)\n",
        "print(count)\n",
        "\n",
        "#selecting 100 as seq_len based on above data\n",
        "seq_len = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDsFP1VIgYzc",
        "outputId": "a7b63b19-57a3-40bd-f8e2-8cb27c8f3952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1349, 1409, 2236, 3336, 7985]\n",
            "8655\n",
            "8655\n",
            "comment popped []\n",
            "author popped 1\n",
            "comment popped []\n",
            "author popped 1\n",
            "comment popped []\n",
            "author popped 1\n",
            "comment popped []\n",
            "author popped 1\n",
            "comment popped []\n",
            "author popped 1\n",
            "8650\n",
            "8650\n",
            "padded features  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14088, 10241, 14569, 11818, 5539, 15284, 7977, 13186, 12931, 15891, 15081, 13329, 14820, 15284, 1725, 7434, 10383, 2708, 3138, 661, 1003], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5602, 1288, 3208, 901], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4990, 2293, 7828, 13568, 6358, 9594, 11303, 1291, 10241, 3301, 2244, 9455, 14744, 12558, 4598, 15179, 1602, 9541, 16015, 14820, 12823, 13568, 3565, 4926, 8387, 6998, 10438, 3621, 3138, 15347], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4598, 919, 12415], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6163, 799, 13716, 16236, 16229, 15347, 15080, 15229, 583, 12443, 10241, 14520, 12558, 12984, 3190, 4990, 12888, 2644, 10835, 14820, 3138, 799, 13716, 5040, 16229, 15347, 15274, 10241, 5596, 583, 946], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7661, 1153, 10241, 12951, 5747, 9495, 10241, 15988, 2721, 4088, 10299, 6646, 4476, 3138, 9780, 2264, 10383, 7870, 4607, 9541, 10494, 9454, 14788, 13262, 11070, 13568], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15507, 3138, 1725, 14202, 7198, 12820, 9268], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6770, 10241, 5624, 1530, 12558, 7750, 2222, 8051, 8447, 15081, 13921, 4749, 12032, 1546, 540, 16203, 2194, 14744, 1291, 1201, 1647, 9541, 12931, 8447, 14087, 4457, 540, 10169, 2250, 3897, 2708, 10312, 9541, 8447, 11174, 475, 14820, 1990, 6344, 9930, 7418, 9541, 475, 14820, 12558, 10241, 3897, 12931, 6740, 9541, 13081, 3475, 15262, 10241, 5584, 540, 8422, 1330, 9541, 5539, 6021, 5296], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15935, 2392, 13112, 14197, 12594, 15089, 8297, 12213, 6122, 6122, 6122, 6122, 1529, 9498], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12699, 11670, 14820, 14142, 9780, 11429, 6640, 540, 7828, 9495, 10342, 8381, 540, 14604, 6077, 5539, 15081, 10241, 14572, 11873, 15415, 1374, 12888, 14329, 15686, 14820, 3993, 12558, 11776, 16190, 10241, 10342, 1291, 11670, 3138, 13125, 14905, 8246, 11670, 5539, 15272, 10241, 12518]]\n"
          ]
        }
      ],
      "source": [
        "# Removing zero length msgs and truncating/padding comments with zeros to get\n",
        "# even length comments\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "# removing zero length integar comments and their corresponding authors\n",
        "zero_length_comment_indices = []\n",
        "\n",
        "for comment_index in range(len(int_tokenized_comments_list)):\n",
        "  if (len(int_tokenized_comments_list[comment_index]))==0:\n",
        "    zero_length_comment_indices.append(comment_index)\n",
        "\n",
        "print(zero_length_comment_indices)\n",
        "print(len(int_tokenized_comments_list))\n",
        "print(len(int_comment_authors_list))\n",
        "\n",
        "#sorting zero length comments in descending order so that poping elements does\n",
        "#not change indices of other zero length comments\n",
        "zero_length_comment_indices.sort(reverse=True)\n",
        "\n",
        "for comment_index in zero_length_comment_indices:\n",
        "  print(\"comment popped\", int_tokenized_comments_list.pop(comment_index))\n",
        "  print(\"author popped\", int_comment_authors_list.pop(comment_index))\n",
        "\n",
        "print(len(int_tokenized_comments_list))\n",
        "print(len(int_comment_authors_list))\n",
        "\n",
        "# todo: change name of variable from start\n",
        "target_labels = int_comment_authors_list\n",
        "\n",
        "padded_features = []\n",
        "\n",
        "for comment in int_tokenized_comments_list:\n",
        "  padded_comment = [0]*seq_len\n",
        "  if len(comment)<=seq_len:\n",
        "    padded_comment[seq_len-len(comment):] = comment[:]\n",
        "  else:\n",
        "    padded_comment[:] = comment[:seq_len]\n",
        "  \n",
        "  padded_features.append(padded_comment)\n",
        "\n",
        "print(\"padded features \", padded_features[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYKoEWJEHvYs",
        "outputId": "c0a98224-7a4c-4101-bb17-b975ea9c426c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 100) (6920,)\n",
            "(865, 100) (865,)\n",
            "(865, 100) (865,)\n"
          ]
        }
      ],
      "source": [
        "# Divide into test, train and validate sets\n",
        "# and covert to numpy arrays\n",
        "#-------------------------------------------------\n",
        "import numpy\n",
        "\n",
        "split_frac = 0.8\n",
        "split_indx = int(split_frac*len(padded_features))\n",
        "\n",
        "train_x, remaining_x = numpy.array(padded_features[:split_indx]), padded_features[split_indx:]\n",
        "train_y, remaining_y = numpy.array(target_labels[:split_indx]), target_labels[split_indx:]\n",
        "\n",
        "test_val_frac = 0.5\n",
        "test_val_indx = int(test_val_frac*len(remaining_x))\n",
        "\n",
        "test_x, vald_x = numpy.array(remaining_x[:test_val_indx]), numpy.array(remaining_x[:test_val_indx])\n",
        "test_y, vald_y = numpy.array(remaining_y[:test_val_indx]), numpy.array(remaining_y[:test_val_indx])\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "print(vald_x.shape, vald_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF8sgzgqFyjD",
        "outputId": "b5015033-b146-4a45-84f1-32f1705f63a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# creating tensor dataloaders \n",
        "#\n",
        "#########################################\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "vald_data = TensorDataset(torch.from_numpy(vald_x), torch.from_numpy(vald_y))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(vald_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print(sample_x.size(), sample_y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_OzU8MpPJN4l"
      },
      "outputs": [],
      "source": [
        "# creating LSTM neural network\n",
        "#\n",
        "######################################\n",
        "import torch.nn as nn\n",
        "\n",
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "class AuthorIdentification(nn.Module):\n",
        "\n",
        "  def __init__(self, output_size, n_layers, hidden_dim, vocab_size, embedding_dim, drop_prob=0.5):\n",
        "    super(AuthorIdentification, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    #embedding and lstm layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "    \n",
        "    #drop out layer\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    #linear layer\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    x = x.long()\n",
        "    embeds = self.embedding(x)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "    lstm_out =  lstm_out[:,-1,:] #getting the last time stepout\n",
        "\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    #create two new tensors with dimension n_layer x batch_size x hidden_dim,\n",
        "    #initialized to zero, for hidden state and cell state of lstm\n",
        "\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "               weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "               weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "    return hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMzcT2Z4PrGV",
        "outputId": "a822c480-f3f7-4f5f-9c5a-561255b5b0ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AuthorIdentification(\n",
            "  (embedding): Embedding(16295, 400)\n",
            "  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Instantiate the model with hyperparameters\n",
        "\n",
        "vocab_size = len(all_words)+1 # +1 for the padded zero\n",
        "output_size = 3\n",
        "embedding_dim = 400\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "net = AuthorIdentification(output_size, n_layers, hidden_dim, vocab_size, embedding_dim)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmtsZf8DSvfz"
      },
      "outputs": [],
      "source": [
        "# Let the training begin!\n",
        "#\n",
        "########################################\n",
        "import torch\n",
        "\n",
        "#loss and optimization functions\n",
        "\n",
        "lr = 0.00006\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MultiMarginLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-arsjgmPjPz",
        "outputId": "f4146abc-b6e9-48a2-8d70-216c8df7d55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:175: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [32, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 100... Loss: 0.669105... Val Loss: 0.655003...\n",
            "Epoch: 1/20... Step: 200... Loss: 0.631179... Val Loss: 0.630441...\n",
            "Epoch: 2/20... Step: 300... Loss: 0.571465... Val Loss: 0.611588...\n",
            "Epoch: 2/20... Step: 400... Loss: 0.625084... Val Loss: 0.594783...\n",
            "Epoch: 3/20... Step: 500... Loss: 0.593477... Val Loss: 0.568086...\n",
            "Epoch: 3/20... Step: 600... Loss: 0.543577... Val Loss: 0.543356...\n",
            "Epoch: 4/20... Step: 700... Loss: 0.486937... Val Loss: 0.531375...\n",
            "Epoch: 4/20... Step: 800... Loss: 0.570336... Val Loss: 0.515077...\n",
            "Epoch: 5/20... Step: 900... Loss: 0.422605... Val Loss: 0.498140...\n",
            "Epoch: 5/20... Step: 1000... Loss: 0.494463... Val Loss: 0.478303...\n",
            "Epoch: 6/20... Step: 1100... Loss: 0.398685... Val Loss: 0.466584...\n",
            "Epoch: 6/20... Step: 1200... Loss: 0.592785... Val Loss: 0.457393...\n",
            "Epoch: 7/20... Step: 1300... Loss: 0.497938... Val Loss: 0.444745...\n",
            "Epoch: 7/20... Step: 1400... Loss: 0.320680... Val Loss: 0.442378...\n",
            "Epoch: 7/20... Step: 1500... Loss: 0.519998... Val Loss: 0.439967...\n",
            "Epoch: 8/20... Step: 1600... Loss: 0.366724... Val Loss: 0.428258...\n",
            "Epoch: 8/20... Step: 1700... Loss: 0.521880... Val Loss: 0.423340...\n",
            "Epoch: 9/20... Step: 1800... Loss: 0.340554... Val Loss: 0.432948...\n",
            "Epoch: 9/20... Step: 1900... Loss: 0.348781... Val Loss: 0.424668...\n",
            "Epoch: 10/20... Step: 2000... Loss: 0.487358... Val Loss: 0.415069...\n",
            "Epoch: 10/20... Step: 2100... Loss: 0.253917... Val Loss: 0.420434...\n",
            "Epoch: 11/20... Step: 2200... Loss: 0.211626... Val Loss: 0.407471...\n",
            "Epoch: 11/20... Step: 2300... Loss: 0.188197... Val Loss: 0.414660...\n",
            "Epoch: 12/20... Step: 2400... Loss: 0.319312... Val Loss: 0.403970...\n",
            "Epoch: 12/20... Step: 2500... Loss: 0.257753... Val Loss: 0.400210...\n",
            "Epoch: 13/20... Step: 2600... Loss: 0.356772... Val Loss: 0.393261...\n",
            "Epoch: 13/20... Step: 2700... Loss: 0.136613... Val Loss: 0.392942...\n",
            "Epoch: 13/20... Step: 2800... Loss: 0.314478... Val Loss: 0.383187...\n",
            "Epoch: 14/20... Step: 2900... Loss: 0.181888... Val Loss: 0.387906...\n",
            "Epoch: 14/20... Step: 3000... Loss: 0.151149... Val Loss: 0.393276...\n",
            "Epoch: 15/20... Step: 3100... Loss: 0.261187... Val Loss: 0.385495...\n",
            "Epoch: 15/20... Step: 3200... Loss: 0.275090... Val Loss: 0.386253...\n",
            "Epoch: 16/20... Step: 3300... Loss: 0.190700... Val Loss: 0.385659...\n",
            "Epoch: 16/20... Step: 3400... Loss: 0.199873... Val Loss: 0.380665...\n",
            "Epoch: 17/20... Step: 3500... Loss: 0.294527... Val Loss: 0.379867...\n",
            "Epoch: 17/20... Step: 3600... Loss: 0.114744... Val Loss: 0.380145...\n",
            "Epoch: 18/20... Step: 3700... Loss: 0.222372... Val Loss: 0.378817...\n",
            "Epoch: 18/20... Step: 3800... Loss: 0.082555... Val Loss: 0.394614...\n",
            "Epoch: 19/20... Step: 3900... Loss: 0.192984... Val Loss: 0.383975...\n",
            "Epoch: 19/20... Step: 4000... Loss: 0.123564... Val Loss: 0.386600...\n",
            "Epoch: 19/20... Step: 4100... Loss: 0.147141... Val Loss: 0.391188...\n",
            "Epoch: 20/20... Step: 4200... Loss: 0.170621... Val Loss: 0.391926...\n",
            "Epoch: 20/20... Step: 4300... Loss: 0.257537... Val Loss: 0.396758...\n"
          ]
        }
      ],
      "source": [
        "#training parameters\n",
        "\n",
        "epochs = 20  #loss stops decreasing after this point\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5 #gradient clipping\n",
        "\n",
        "if (train_on_gpu):\n",
        "  net.cuda()\n",
        "\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "  #initialize hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  #batch loop\n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    #creating new variables for hidden state, otherwise we will backprop\n",
        "    #through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "    \n",
        "    #zero accumulated gradients\n",
        "    net.zero_grad()\n",
        "\n",
        "    #get the output from the model\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    #calculate the loss and perform the backprop\n",
        "    loss = criterion(output.squeeze(), labels)\n",
        "    loss.backward()\n",
        "\n",
        "    #clip gradient helps prevent exploding gradient problem in LSTMs and RNNs\n",
        "    nn.utils.clip_grad_norm_(net.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    # loss stats\n",
        "    if counter % print_every == 0:\n",
        "      # Get validation loss\n",
        "      val_h = net.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      net.eval()\n",
        "      for inputs, labels in valid_loader:\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        output, val_h = net(inputs, val_h)\n",
        "        val_loss = criterion(output.squeeze(), labels)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "      net.train()\n",
        "      print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "            \"Step: {}...\".format(counter),\n",
        "            \"Loss: {:.6f}...\".format(loss.item()),\n",
        "            \"Val Loss: {:.6f}...\".format(numpy.mean(val_losses)))\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting test loss and accuracy\n",
        "# \n",
        "##################################\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "total_uc = 0 # total unclassified-if none of categories has more than 0.5 probability\n",
        "\n",
        "\n",
        "def one_hot_decode(encoded):\n",
        "  decoded = []\n",
        "  #print(\"length of encoded array: \", len(encoded))\n",
        "  for code in encoded:\n",
        "    #print(code)\n",
        "    index_array = numpy.nonzero(code)\n",
        "    #print(index_array[0])\n",
        "    if len(index_array[0])!=0:\n",
        "      decoded.append(index_array[0][0])\n",
        "    else:\n",
        "      decoded.append(-1)\n",
        "  \n",
        "  #decoded = [numpy.nonzero(code)[0][0] for code in encoded if len(numpy.nonzero(code))!=0]\n",
        "  #print(\"length of decoded array:\", len(decoded))\n",
        "  #print(decoded)\n",
        "  return decoded\n",
        "\n",
        "\n",
        "#initialize hidden\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "#iterating over the test set\n",
        "for inputs, labels in test_loader:\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  if train_on_gpu:\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "  output, h = net(inputs, h)\n",
        "\n",
        "  test_loss = criterion(output.squeeze(), labels)\n",
        "  test_losses.append(test_loss.item())\n",
        "\n",
        "  p = F.softmax(output, dim=1).data\n",
        "\n",
        "  if train_on_gpu:\n",
        "    p = p.cpu()\n",
        "\n",
        "  # getting the index with highest probability - this will also be the pred label\n",
        "  #pred = torch.argmax(p, dim=1)\n",
        "  pred = torch.round(p)\n",
        "  pred_classes = torch.from_numpy(numpy.array(one_hot_decode(pred.numpy())))\n",
        "  correct_tensor = pred_classes.eq(labels.cpu())\n",
        "  correct = numpy.squeeze(correct_tensor.numpy()) if not train_on_gpu else numpy.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += numpy.sum(correct)\n",
        "\n",
        "  #getting total unclassified outputs\n",
        "  uc_indices = torch.where(pred_classes == -1)\n",
        "  total_uc += len(uc_indices[0])\n",
        "\n",
        "\n",
        "print(\"Total correct\", num_correct)\n",
        "print(\"total unclassified\", total_uc)\n",
        "print(\"Total test\", len(test_loader.dataset))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/(len(test_loader.dataset)-total_uc)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
        "print(\"Test losses: {:.6f}\".format(numpy.mean(val_losses)))\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxE3EgH5spcz",
        "outputId": "ba7eaf39-4463-47cb-98b6-21e5e8973b3e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total correct 490\n",
            "total unclassified 146\n",
            "Total test 865\n",
            "Test accuracy: 0.682\n",
            "Test losses: 0.396758\n"
          ]
        }
      ]
    }
  ]
}